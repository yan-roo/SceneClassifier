{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiF_Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx1UyhvzZN3GpAfi5MCp2L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bYODQLKqSlbK","colab_type":"text"},"source":["# Load the pre-trained BiT model"]},{"cell_type":"markdown","metadata":{"id":"9In7YnN7Sz2Q","colab_type":"text"},"source":["### Where to find the models\n","\n","Models that output image features (pre-logit layer) can be found at\n","* `https://tfhub.dev/google/bit/m-{archi, e.g. r50x1}/1`\n","\n","whereas models that return outputs in the Imagenet  (ILSVRC-2012) label space can be found at **bold text**\n","\n","* `https://tfhub.dev/google/bit/m-{archi, e.g. r50x1}/ilsvrc2012_classification/1`\n","\n","The architectures we have include R50x1, R50x3, R101x1, R101x3 and R152x4. The architectures are all in lowercase in the links."]},{"cell_type":"code","metadata":{"id":"OswVaQ87Rhtn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741149405,"user_tz":-480,"elapsed":22667,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}}},"source":["import numpy as np\n","import tensorflow_hub\n","# Load model into KerasLayer\n","module = tensorflow_hub.KerasLayer(\"https://tfhub.dev/google/bit/m-r50x1/1\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjV3a6CnTZTU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741149408,"user_tz":-480,"elapsed":22659,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}}},"source":["# Hyperparameters\n","batch_size = 64 # Training batch size\n","num_classes = 15  # Classes in dataset\n","num_epochs = 40   # Epochs for training   "],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YJqq9-eTKog","colab_type":"text"},"source":["#### Add new head to the BiT model\n","\n","Since we want to use BiT on a new dataset (not the one it was trained on), we need to replace the final layer with one that has the correct number of output classes. This final layer is called the head.\n","\n","Note that it is important to **initialise the new head to all zeros**."]},{"cell_type":"code","metadata":{"id":"sEVqMpZLRvQL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741149410,"user_tz":-480,"elapsed":22652,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}}},"source":["import tensorflow as tf\n","\n","class MyBiTModel(tf.keras.Model):\n","  \"\"\"BiT with a new head.\"\"\"\n","\n","  def __init__(self, num_classes, module):\n","    super().__init__()\n","\n","    self.num_classes = num_classes\n","    self.head = tf.keras.layers.Dense(num_classes, kernel_initializer='zeros')\n","    self.bit_model = module\n","  \n","  def call(self, images):\n","    # No need to cut head off since we are using feature extractor model\n","    bit_embedding = self.bit_model(images)\n","    return self.head(bit_embedding)\n","\n","model = MyBiTModel(num_classes=num_classes, module=module)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HW8AyCWATZ7g","colab_type":"text"},"source":["### Data and preprocessing"]},{"cell_type":"code","metadata":{"id":"rppySJigR6ax","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1594741149412,"user_tz":-480,"elapsed":22647,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}},"outputId":"a473159b-7f04-4899-dc8d-67d41e9e9fc1"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive') # 將 google drive 掛載在 colob，\n","%cd gdrive/My Drive/Colab Notebooks"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ec38DsavTnL4","colab_type":"text"},"source":["If Last layer (Dense) activation=None\n","\n","Using 'sparse' class_mode to fit the pretrain weight.\n","\n","Also, the loss should be 'SparseCategoricalCrossentropy'\n","\n","\n","---\n","\n","If Last layer (Dense) activation=softmax\n","\n","Using 'categorical' class_mode to fit the pretrain weight.\n","\n","Also, the loss should be 'categorical_crossentropy'"]},{"cell_type":"code","metadata":{"id":"Qw9hoGUuSnDx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594741149413,"user_tz":-480,"elapsed":22638,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}},"outputId":"1973a482-7080-491b-f491-efd9927d6849"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# preprocessing image and divide validaiton set\n","train_datagen=ImageDataGenerator(horizontal_flip=True, brightness_range=[0.5,1.5], zoom_range=[0.8,1], rescale=1/255)\n","\n","train_generator=train_datagen.flow_from_directory('hw5_data/train/',\n","                                                 target_size=(256,256),\n","                                                 batch_size=batch_size,\n","                                                 class_mode='sparse',\n","                                                 shuffle=True,\n","                                                 subset='training')\n","\n","validation_datagen=ImageDataGenerator(rescale=1/255)\n","\n","validation_generator = validation_datagen.flow_from_directory('hw5_data/test/',target_size=(256,256),\n","                                                 batch_size=batch_size,\n","                                                 class_mode='sparse')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Found 1500 images belonging to 15 classes.\n","Found 150 images belonging to 15 classes.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"AuU_lyZoP4W9","colab_type":"text"},"source":["**Hyperparameter heuristic details**\n","\n","In BiT-HyperRule, we use a vanilla SGD optimiser with an initial learning rate of 0.003, momentum 0.9 and batch size 512. We decay the learning rate by a factor of 10 at 30%, 60% and 90% of the training steps. \n","\n","As data preprocessing, we resize the image, take a random crop, and then do a random horizontal flip (details in table below). We do random crops and horizontal flips for all tasks except those where such actions destroy label semantics. E.g. we don’t apply random crops to counting tasks, or random horizontal flip to tasks where we’re meant to predict the orientation of an object."]},{"cell_type":"code","metadata":{"id":"55IFOmR1U4EO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741149417,"user_tz":-480,"elapsed":22632,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}}},"source":["# Define optimiser and loss\n","\n","lr = 0.003 * batch_size / 512 \n","\n","# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n","step_size_train = train_generator.n // train_generator.batch_size\n","total_step = num_epochs*step_size_train\n","\n","lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[int(total_step*0.3), int(total_step*0.6), int(total_step*0.9)], \n","                                                                   values=[lr, lr*0.1, lr*0.001, lr*0.0001])\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iPGH3o-HVn51","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594742764153,"user_tz":-480,"elapsed":1637358,"user":{"displayName":"王彥儒","photoUrl":"","userId":"15727579035001839763"}},"outputId":"2dc813ce-2856-4875-9e9d-06d68bf7fd40"},"source":["loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer=optimizer,\n","              loss=loss_fn,\n","              metrics=['accuracy'])\n","\n","history = model.fit_generator(generator=train_generator, validation_data=validation_generator,\n","                              epochs=num_epochs)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-7-b194da8a194d>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-7-b194da8a194d>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/40\n","24/24 [==============================] - 40s 2s/step - loss: 1.1726 - accuracy: 0.7413 - val_loss: 0.2835 - val_accuracy: 0.9133\n","Epoch 2/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.2188 - accuracy: 0.9433 - val_loss: 0.2197 - val_accuracy: 0.9200\n","Epoch 3/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.1609 - accuracy: 0.9533 - val_loss: 0.1790 - val_accuracy: 0.9467\n","Epoch 4/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.1419 - accuracy: 0.9593 - val_loss: 0.1656 - val_accuracy: 0.9600\n","Epoch 5/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.1148 - accuracy: 0.9713 - val_loss: 0.1695 - val_accuracy: 0.9467\n","Epoch 6/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.1069 - accuracy: 0.9740 - val_loss: 0.1497 - val_accuracy: 0.9533\n","Epoch 7/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.1015 - accuracy: 0.9760 - val_loss: 0.1520 - val_accuracy: 0.9400\n","Epoch 8/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0889 - accuracy: 0.9787 - val_loss: 0.1399 - val_accuracy: 0.9667\n","Epoch 9/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0851 - accuracy: 0.9813 - val_loss: 0.1390 - val_accuracy: 0.9600\n","Epoch 10/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0763 - accuracy: 0.9833 - val_loss: 0.1352 - val_accuracy: 0.9667\n","Epoch 11/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0782 - accuracy: 0.9807 - val_loss: 0.1326 - val_accuracy: 0.9667\n","Epoch 12/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0758 - accuracy: 0.9833 - val_loss: 0.1351 - val_accuracy: 0.9467\n","Epoch 13/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0685 - accuracy: 0.9860 - val_loss: 0.1333 - val_accuracy: 0.9533\n","Epoch 14/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0675 - accuracy: 0.9880 - val_loss: 0.1312 - val_accuracy: 0.9600\n","Epoch 15/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0670 - accuracy: 0.9873 - val_loss: 0.1323 - val_accuracy: 0.9600\n","Epoch 16/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0653 - accuracy: 0.9840 - val_loss: 0.1322 - val_accuracy: 0.9667\n","Epoch 17/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0619 - accuracy: 0.9900 - val_loss: 0.1332 - val_accuracy: 0.9667\n","Epoch 18/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0625 - accuracy: 0.9887 - val_loss: 0.1333 - val_accuracy: 0.9667\n","Epoch 19/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0632 - accuracy: 0.9880 - val_loss: 0.1330 - val_accuracy: 0.9667\n","Epoch 20/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0641 - accuracy: 0.9900 - val_loss: 0.1327 - val_accuracy: 0.9667\n","Epoch 21/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0651 - accuracy: 0.9860 - val_loss: 0.1328 - val_accuracy: 0.9667\n","Epoch 22/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0632 - accuracy: 0.9893 - val_loss: 0.1314 - val_accuracy: 0.9667\n","Epoch 23/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0711 - accuracy: 0.9840 - val_loss: 0.1309 - val_accuracy: 0.9667\n","Epoch 24/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0617 - accuracy: 0.9873 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 25/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0587 - accuracy: 0.9907 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 26/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0603 - accuracy: 0.9900 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 27/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0645 - accuracy: 0.9867 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 28/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0641 - accuracy: 0.9860 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 29/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0628 - accuracy: 0.9873 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 30/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0626 - accuracy: 0.9873 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 31/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0596 - accuracy: 0.9900 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 32/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0619 - accuracy: 0.9867 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 33/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0595 - accuracy: 0.9893 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 34/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0631 - accuracy: 0.9887 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 35/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0625 - accuracy: 0.9867 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 36/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0646 - accuracy: 0.9893 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 37/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0560 - accuracy: 0.9933 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 38/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0600 - accuracy: 0.9860 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 39/40\n","24/24 [==============================] - 38s 2s/step - loss: 0.0592 - accuracy: 0.9913 - val_loss: 0.1305 - val_accuracy: 0.9667\n","Epoch 40/40\n","24/24 [==============================] - 37s 2s/step - loss: 0.0633 - accuracy: 0.9880 - val_loss: 0.1305 - val_accuracy: 0.9667\n"],"name":"stdout"}]}]}